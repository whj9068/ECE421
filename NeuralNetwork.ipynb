{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5MvjIMJBnC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "02e881f1-6f70-401e-81c0-4b8e4e74d562"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix is from Part 1a is:  [[ 7.  1.]\n",
            " [ 0. 12.]]\n",
            "Confusion Matrix from Part 1b is: [[ 7  1]\n",
            " [ 1 11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def fit_NeuralNetwork(X_train,y_train,alpha,hidden_layer_sizes,epochs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        X_train: X train is the training dataset that is composed of N, d-dimensional vectors.\n",
        "        y_train: y train is an N − dimensional vector that consists of the corresponding class observations for each training vector in X train.\n",
        "        alpha: alpha is the step-size used to update the weights in the NN.\n",
        "        hidden_layer_sizes: hidden layer sizes contains information about the number of nodes in each hidden layer\n",
        "        epochs: epochs represents the number of times the training process will pass through the entire training set to tune the weight parameters.\n",
        "    Outputs:\n",
        "        err: err is a list that contains the average error computed at each epoch.\n",
        "        weights: weights contains the final weights obtained from training the NN using the back propagation algorithm.\n",
        "    \"\"\"\n",
        "    # Initialize the epoch errors\n",
        "    err=np.zeros((epochs,1))\n",
        "\n",
        "    # Initialize the architecture\n",
        "    N, d = X_train.shape\n",
        "    X0 = np.ones((N,1))\n",
        "    X_train = np.hstack((X0,X_train))\n",
        "    d=d+1\n",
        "    L = len(hidden_layer_sizes)\n",
        "    L=L+2\n",
        "\n",
        "    #Initializing the weights for input layer\n",
        "    weight_layer = np.random.normal(0, 0.1, (d,hidden_layer_sizes[0])) #np.ones((d,hidden_layer_sizes[0]))\n",
        "    weights = []\n",
        "    weights.append(weight_layer) #append(0.1*weight_layer)\n",
        "\n",
        "    #Initializing the weights for hidden layers\n",
        "    for l in range(L-3):\n",
        "        weight_layer = np.random.normal(0, 0.1, (hidden_layer_sizes[l]+1,hidden_layer_sizes[l+1]))\n",
        "        weights.append(weight_layer)\n",
        "\n",
        "    #Initializing the weights for output layers\n",
        "    weight_layer= np.random.normal(0, 0.1, (hidden_layer_sizes[l+1]+1,1))\n",
        "    weights.append(weight_layer)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        choiceArray=np.arange(0, N)\n",
        "        np.random.shuffle(choiceArray)\n",
        "        errN=0\n",
        "        for n in range(N):\n",
        "            index=choiceArray[n]\n",
        "            x=np.transpose(X_train[index])\n",
        "            #Model Update: Forward Propagation, Backpropagation\n",
        "            # update the weight and calculate the error\n",
        "            X,S = forwardPropagation(x,weights)\n",
        "            g = backPropagation(X,y_train[index],S,weights)\n",
        "            weights = updateWeights(weights, g, alpha)\n",
        "            errN += errorPerSample(X, y_train[index])\n",
        "        err[e]=errN/N\n",
        "    return err, weights\n",
        "\n",
        "def forwardPropagation(x, weights):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x: The input x is d + 1 dimensional. With 1st component of x is 1, and remaining d components represent input feature vector xn\n",
        "        weights: weights is a list where each element l is a matrix representing the weights of edges between layer l and l + 1\n",
        "    Outputs:\n",
        "        retX: X is the output at all nodes residing in all layers of the NN. X is a list where the l thelement is a vector of size dl+ 1\n",
        "        retS: S is the input into all nodes located in all layers of the NN, composed of L − 1 elements\n",
        "    \"\"\"\n",
        "    l=len(weights)+1\n",
        "    currX = x\n",
        "    retS=[]\n",
        "    retX=[]\n",
        "    retX.append(currX)\n",
        "\n",
        "    # Forward Propagate for each layer\n",
        "    for i in range(l-1):\n",
        "\n",
        "        currS=np.dot(np.transpose(weights[i]),currX) #Dot product between the layer and the weight matrix\n",
        "        retS.append(currS)\n",
        "        currX=currS\n",
        "        if i != len(weights)-1:\n",
        "            for j in range(len(currS)):\n",
        "                currX[j]= activation(currS[j]) #\n",
        "            currX= np.hstack((1,currX))\n",
        "        else:\n",
        "            currX= outputf(currS) #Apply the output activation\n",
        "        retX.append(currX)\n",
        "    return retX,retS\n",
        "\n",
        "def errorPerSample(X,y_n):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        X: the output RetX of forward propagation\n",
        "        y_n: class label observed for the input vector x n.\n",
        "    Outputs:\n",
        "        eN: This is the error contributed at the last layer x L. This is a single-dimensional output.\n",
        "    \"\"\"\n",
        "    #eN is the errorf from the last layer\n",
        "    eN = errorf(X[len(X)-1],y_n)\n",
        "    return eN\n",
        "\n",
        "def backPropagation(X,y_n,s,weights):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        X: the output RetX of forward propagation\n",
        "        y_n: class label observed for the input vector x n.\n",
        "        s: the output RetS of forward propagation\n",
        "        weights: weights is a list where each element l is a matrix representing the weights of edges between layer l and l + 1\n",
        "    Outputs:\n",
        "        g: This function will output a list g which contains all the error gradients computed for all the weights in the NN.\n",
        "    \"\"\"\n",
        "    #x:0,1,...,L\n",
        "    #S:1,...,L\n",
        "    #weights: 1,...,L\n",
        "    l=len(X)\n",
        "    delL=[]\n",
        "\n",
        "    # To be able to complete this function, you need to understand this line below\n",
        "    # In this line, we are computing the derivative of the Loss function w.r.t the\n",
        "    # output layer (without activation). This is dL/dS[l-2]\n",
        "    # By chain rule, dL/dS[l-2] = dL/dy * dy/dS[l-2] . Now dL/dy is the derivative Error and\n",
        "    # dy/dS[l-2]  is the derivative output.\n",
        "    delL.insert(0,derivativeError(X[l-1],y_n)*derivativeOutput(s[l-2]))\n",
        "    curr=0\n",
        "\n",
        "    # Now, let's calculate dL/dS[l-2], dL/dS[l-3],...\n",
        "    for i in range(len(X)-2, 0, -1): #L-1,...,0\n",
        "        delNextLayer=delL[curr]\n",
        "        WeightsNextLayer=weights[i]\n",
        "        sCurrLayer=s[i-1]\n",
        "\n",
        "        #Init this to 0s vector\n",
        "        delN=np.zeros((len(s[i-1]),1))\n",
        "\n",
        "        #Now we calculate the gradient backward\n",
        "        #Remember: dL/dS[i] = dL/dS[i+1] * W(which W???) * activation\n",
        "        for j in range(len(s[i-1])): #number of nodes in layer i - 1\n",
        "            for k in range(len(s[i])): #number of nodes in layer i\n",
        "                #calculate delta at node j\n",
        "                delN[j]=delN[j]+ WeightsNextLayer[j][k] * delNextLayer[k] * derivativeActivation(sCurrLayer[j])\n",
        "\n",
        "        delL.insert(0,delN)\n",
        "\n",
        "    # We have all the deltas we need. Now, we need to find dL/dW.\n",
        "    # It's very simple now, dL/dW = dL/dS * dS/dW = dL/dS * X\n",
        "    g=[]\n",
        "    for i in range(len(delL)):\n",
        "        rows,cols=weights[i].shape\n",
        "        gL=np.zeros((rows,cols))\n",
        "        currX=X[i]\n",
        "        currdelL=delL[i]\n",
        "        for j in range(rows):\n",
        "            for k in range(cols):\n",
        "                #Calculate the gradient using currX and currdelL\n",
        "                gL[j,k]= np.dot(currX[j],currdelL[k])\n",
        "        g.append(gL)\n",
        "    return g\n",
        "\n",
        "def updateWeights(weights,g,alpha):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        weights: weights represent the current weights assigned to all edges in the NN.\n",
        "        g: g is a list that contains all the gradients of the weights as computed by the backPropagation function.\n",
        "        alpha: alpha is the step-size of the weight update.\n",
        "    Outputs:\n",
        "        nW: nW will contain the updated weights.\n",
        "    \"\"\"\n",
        "    nW=[]\n",
        "    for i in range(len(weights)):\n",
        "        rows, cols = weights[i].shape\n",
        "        currWeight=weights[i]\n",
        "        currG=g[i]\n",
        "        for j in range(rows):\n",
        "            for k in range(cols):\n",
        "                #Gradient Descent Update\n",
        "                currWeight[j,k]= currWeight[j,k] - alpha*currG[j][k]#\n",
        "        nW.append(currWeight)\n",
        "    return nW\n",
        "\n",
        "def activation(s):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "     s: single-dimensional real-valued number.\n",
        "  Output:\n",
        "     single-dimensional output of performing a ReLU operation on the input.\n",
        "  \"\"\"\n",
        "  return relu(s)\n",
        "\n",
        "def relu(x):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      x: single-dimensional real-valued number.\n",
        "  Output:\n",
        "      performing a ReLU operation on the input.\n",
        "  \"\"\"\n",
        "  return max(0.0,x)\n",
        "\n",
        "def derivativeActivation(s):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      s: The input to this function is a single-dimensional real-valued number.\n",
        "  Output:\n",
        "      The output is the derivative of the activation function θ(s).\n",
        "  \"\"\"\n",
        "  if s<=0:\n",
        "    return 0\n",
        "  elif s>0:\n",
        "    return 1\n",
        "\n",
        "def outputf(s):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      s: The input to this function is a single-dimensional real-valued number.\n",
        "  Output:\n",
        "      The output of this function is a single-dimensional value which is evaluated using the logistic regression function..\n",
        "  \"\"\"\n",
        "  return (1/(1+np.exp(-s)))\n",
        "\n",
        "def derivativeOutput(s):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      s： The input to this function is a single-dimensional real-valued number.\n",
        "  Output:\n",
        "      The output of this function is derivative of the sigmoid function evaluated at s\n",
        "  \"\"\"\n",
        "  return ((np.exp(s))/((np.exp(s)+1)**2))\n",
        "\n",
        "def errorf(x_L,y):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      x_L: The input to this function is a single-dimensional real-valued number.\n",
        "        (output from the NN model which is obtained by applying forward propagation to xn)\n",
        "      y: The input to this function is a single-dimensional discrete variable which takes values in the set { +1,-1 }\n",
        "        (class that the training data point x nbelongs to)\n",
        "  Outputs:\n",
        "      The output of this function is evaluated via the log loss error function\n",
        "  \"\"\"\n",
        "    #Fill in the return values\n",
        "  if y==1:\n",
        "    return -np.log(x_L)\n",
        "  else:\n",
        "    return -np.log(1-x_L)\n",
        "\n",
        "def derivativeError(x_L,y):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "      x_L: The input to this function is a single-dimensional real-valued number.\n",
        "        (output from the NN model which is obtained by applying forward propagation to xn)\n",
        "      y: The input to this function is a single-dimensional discrete variable which takes values in the set { +1,-1 }\n",
        "        (class that the training data point x nbelongs to)\n",
        "  Outputs:\n",
        "      The output of this function is the derivative of the error function evaluated at x_L\n",
        "  \"\"\"\n",
        "  if y==1:\n",
        "    return -1/x_L\n",
        "  else:\n",
        "    return 1/(1-x_L)\n",
        "\n",
        "def pred(x_n,weights):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x: The input x is d + 1 dimensional. With 1st component of x is 1, and remaining d components represent input feature vector xn\n",
        "        weights: weights is a list where each element l is a matrix representing the weights of edges between layer l and l + 1\n",
        "    Outputs:\n",
        "        c: the class the input belongs to\n",
        "    \"\"\"\n",
        "    #prediction using the forwardPropagation function\n",
        "    retX,retS= forwardPropagation(x_n,weights)\n",
        "    l=len(retX)\n",
        "\n",
        "    # Return -1 if probability lesser than 0.5\n",
        "    # Else return 1\n",
        "    if retX[l-1]<0.5:\n",
        "        return -1\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def confMatrix(X_train,y_train,w):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        X_train: composed of the training points\n",
        "        y_train: composed of the corresponding observations.\n",
        "        w: w is the final weight list computed via the training of the NN\n",
        "    Outputs:\n",
        "        The output of this function will be a four-by-four matrix.\n",
        "    \"\"\"\n",
        "    #This is a copy from PA1\n",
        "    eCount=np.zeros((2,2))\n",
        "    j=0\n",
        "    row, col = X_train.shape\n",
        "    X0 = np.ones((row,1))\n",
        "    X_train = np.hstack((X0,X_train))\n",
        "    for j in range(row):\n",
        "        if (pred(X_train[j],w)==-1 and y_train[j]==-1):\n",
        "            eCount[0,0]=eCount[0,0]+1\n",
        "        elif (pred(X_train[j],w)==1 and y_train[j]==-1):\n",
        "            eCount[0,1]=eCount[0,1]+1\n",
        "        elif (pred(X_train[j],w)==1 and y_train[j]==1):\n",
        "            eCount[1,1]=eCount[1,1]+1\n",
        "        else:\n",
        "            eCount[1,0]=eCount[1,0]+1\n",
        "    return eCount\n",
        "\n",
        "def plotErr(e,epochs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        e: e is the error list containing the average error at each epoch of the training process.\n",
        "        epochs: epochs is a single-dimensional parameter that denotes the number of iterations  through the entire training set to tune the NN weight parameters.\n",
        "    Outputs:\n",
        "        This function will plot the error over the training epochs.\n",
        "    \"\"\"\n",
        "    #Plot the function using plt.plot(...,...,linewidth=2.0)\n",
        "    x = np.arange(epochs)\n",
        "    plt.plot(x,e,linewidth=2.0)\n",
        "    plt.show()\n",
        "\n",
        "def test_SciKit(X_train, X_test, Y_train, Y_test):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        X_train: composed of the training points\n",
        "        X_test: composed of the testing points\n",
        "        Y_train: output observed in the training set\n",
        "        Y_test: output observed in the testing set\n",
        "    Outputs:\n",
        "        cM: This function will output the confusion matrix obtained for the test dataset.\n",
        "    \"\"\"\n",
        "    clf = MLPClassifier(alpha=0.00001, hidden_layer_sizes=(30,10), random_state=1)\n",
        "    clf.fit(X_train,Y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    matrix = confusion_matrix(Y_test, y_pred)\n",
        "    return matrix\n",
        "\n",
        "\n",
        "\n",
        "def test_Part1():\n",
        "    from sklearn.datasets import load_iris\n",
        "    X_train, y_train = load_iris(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train[50:],y_train[50:],test_size=0.2, random_state=1)\n",
        "\n",
        "    for i in range(80):\n",
        "        if y_train[i]==1:\n",
        "            y_train[i]=-1\n",
        "        else:\n",
        "            y_train[i]=1\n",
        "    for j in range(20):\n",
        "        if y_test[j]==1:\n",
        "            y_test[j]=-1\n",
        "        else:\n",
        "            y_test[j]=1\n",
        "\n",
        "    err,w=fit_NeuralNetwork(X_train,y_train,1e-2,[30, 10],100)\n",
        "\n",
        "    plotErr(err,100)\n",
        "\n",
        "    cM=confMatrix(X_test,y_test,w)\n",
        "\n",
        "    sciKit=test_SciKit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    print(\"Confusion Matrix is from Part 1a is: \",cM)\n",
        "    print(\"Confusion Matrix from Part 1b is:\",sciKit)\n",
        "\n",
        "test_Part1()\n"
      ]
    }
  ]
}
