{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsfR_VuCr4ZA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "def load_data():\n",
        "  X =np.load('data2D.npy')\n",
        "  valid_batch = int(len(X) / 3.0)\n",
        "  np.random.seed(45689)\n",
        "  rnd_idx = np.arange(len(X))\n",
        "  np.random.shuffle(rnd_idx)\n",
        "  val_data = X[rnd_idx[:valid_batch]]\n",
        "  data = X[rnd_idx[valid_batch:]]\n",
        "\n",
        "  return data, val_data\n",
        "\n",
        "def truncated_normal(size, threshold=1):\n",
        "  # This function generates a truncated normal distribution\n",
        "  values = truncnorm.rvs(-threshold, threshold, size=size)\n",
        "  return values\n",
        "\n",
        "def distanceFunc(X, MU):\n",
        "  #TODO: explain this function in your report\n",
        "  X1 = torch.unsqueeze(X, -1)\n",
        "  MU1 = torch.unsqueeze(MU.T, 0)\n",
        "\n",
        "  pair_dist = torch.sum((X1 - MU1)**2, 1)\n",
        "  return pair_dist\n",
        "\n",
        "def log_GaussPDF(X, mu, sigma):\n",
        "  #TODO: explain this function in your report\n",
        "  dim = X.shape[-1]\n",
        "\n",
        "  Pi = torch.tensor(float(np.pi))\n",
        "  sigma_2 = (torch.square(sigma)).T # 1 X K\n",
        "  diff = distanceFunc(X, mu)  # N X K\n",
        "\n",
        "  log_PDF = diff / sigma_2  # N X K\n",
        "  log_PDF += dim * torch.log(2 * Pi)\n",
        "  log_PDF += dim * torch.log(sigma_2)\n",
        "  log_PDF *= -0.5  # N X K\n",
        "\n",
        "  return log_PDF\n",
        "\n",
        "def log_posterior(log_PDF, log_pi):\n",
        "  #TODO: Explain this function in your report\n",
        "  log_joint = log_PDF + log_pi.T  # N X K\n",
        "  log_marginal = torch.logsumexp(log_joint,dim=1)  # N x 1\n",
        "\n",
        "  return log_joint, log_marginal\n",
        "\n",
        "def train_gmm(train_data, test_data, k = 5, epoch=1000, init_kmeans=False):\n",
        "  #Load the data\n",
        "  X_train = torch.from_numpy(train_data)\n",
        "  X_test = torch.from_numpy(test_data)\n",
        "\n",
        "  #Initialize logits\n",
        "  if init_kmeans:\n",
        "    logits = torch.ones(k, requires_grad=True)\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=5000).fit(train_data)\n",
        "    mu = torch.tensor(kmeans.cluster_centers_, requires_grad=True)\n",
        "    lr = 0.005\n",
        "  else:\n",
        "    logits = torch.rand(k, requires_grad=True)\n",
        "    mu = torch.randn((k,X_train.shape[1]), requires_grad=True)\n",
        "    lr = 0.005\n",
        "\n",
        "  #Initialize sigma.\n",
        "  sigma = np.abs(truncated_normal((k,1), threshold=1))\n",
        "  sigma = torch.tensor(sigma,requires_grad=True)\n",
        "  optimizer = torch.optim.Adam([logits, mu, sigma], lr=lr,\n",
        "                                betas=(0.9, 0.99), eps=1e-5)\n",
        "  #Train the model\n",
        "  for i in range(epoch):\n",
        "    logpi = F.log_softmax(logits)\n",
        "\n",
        "    #TODO: Compute log likelihood per cluster and marginal likelihood for the whole mixture.\n",
        "    # Fill in the blank (2lines below) of the functions log_GaussPDF and log_posterior\n",
        "    log_PDF = log_GaussPDF(X_train, mu, sigma) #fill in here\n",
        "    _, log_marginal = log_posterior(log_PDF, logpi) #fill in here\n",
        "\n",
        "    #Compute the marginal mean.\n",
        "    loss = -log_marginal.mean()\n",
        "\n",
        "    #Update the paramenters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  #Perform on Test data\n",
        "  logpi = F.log_softmax(logits)\n",
        "\n",
        "  #TODO: Evaluate on Test Set. Fill in the blank of the functions\n",
        "  # log_GaussPDF and log_posterior\n",
        "  log_PDF = log_GaussPDF(X_test, mu, sigma) #fill in here\n",
        "  log_joint_test, log_marginal = log_posterior(log_PDF, logpi) #fill in here\n",
        "  test_loss = -log_marginal.mean()\n",
        "\n",
        "  #Conver to numpy:\n",
        "  test_loss = test_loss.detach().numpy()\n",
        "  log_joint_test = log_joint_test.detach().numpy()\n",
        "  pi = torch.exp(logpi).detach().numpy()\n",
        "  mu = mu.detach().numpy()\n",
        "  sigma = sigma.detach().numpy()\n",
        "\n",
        "  return test_loss, log_joint_test, pi, mu, sigma\n",
        "\n",
        "def test_GMM(k = 5, init_kmeans=False):\n",
        "  train_data, test_data = load_data()\n",
        "  test_loss, log_joint_test, pi, mu, sigma = train_gmm(train_data, test_data, k, init_kmeans=init_kmeans)\n",
        "\n",
        "  index = log_joint_test.argmax(axis=1)\n",
        "  index = index.reshape(len(index), 1)\n",
        "  new_X = np.concatenate((test_data, index), axis = 1)\n",
        "\n",
        "  color_list = ['g', 'b', 'm', 'y', 'c']\n",
        "  for i in range(len(mu)):\n",
        "    tmp = new_X[new_X[...,-1] == i]\n",
        "    plt.scatter(tmp[:,0], tmp[:,1], c=color_list[i])\n",
        "  plt.scatter(mu[:,0], mu[:,1], s=300, c='r', marker = '+')\n"
      ]
    }
  ]
}
